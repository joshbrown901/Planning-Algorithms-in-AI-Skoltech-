{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM SET 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 810
    },
    "id": "VlZaZX-yN7is",
    "outputId": "37f696f6-7bda-4119-ce24-d237f1528529"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import plot_enviroment, action_space, transition_function,probabilistic_transition_function\n",
    "from vi import vi, policy_vi\n",
    "from mdp import mdp, policy_mdp\n",
    "import vi\n",
    "data = np.load('data_ps3.npz')\n",
    "environment = data['environment']\n",
    "x_ini = (11,6)\n",
    "goal = (15,29)\n",
    "im = plot_enviroment(environment,x_ini,goal)\n",
    "plt.matshow(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "M4yimk1JUJwO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "action_space = []\n",
    "action_space.append((-1,0))\n",
    "action_space.append((0,-1))\n",
    "action_space.append((1,0))\n",
    "action_space.append((0,1))\n",
    "def plot_enviroment(env, x, goal):\n",
    "    \"\"\"\n",
    "    env is the grid enviroment\n",
    "    x is the state\n",
    "    \"\"\"\n",
    "    dims = env.shape\n",
    "    current_env = np.copy(env)\n",
    "    current_env[x] = 1.0 \n",
    "    current_env[goal] = 0.3\n",
    "    return current_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_wfuqV-rUU1y"
   },
   "outputs": [],
   "source": [
    "def state_consistency_check(env,x):\n",
    "    \"\"\"Checks wether or not the proposed state is a valid state, i.e. is in colision or our of bounds\"\"\"\n",
    "    \n",
    "    if x[0] < 0 or x[1] < 0 or x[0] >= env.shape[0] or x[1] >= env.shape[1] :\n",
    "        return False\n",
    "    if env[x] >= 1.0-1e-4:\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jYEXK3lpUhIP"
   },
   "outputs": [],
   "source": [
    "def transition_function(env,x,u):\n",
    "    \"\"\"Transition function for states in this problem\n",
    "    x: current state, this is a tuple (i,j)\n",
    "    u: current action, this is a tuple (i,j)\n",
    "    env: enviroment\n",
    "\n",
    "    Output:\n",
    "    new state\n",
    "    True if correctly propagated\n",
    "    False if this action can't be executed\n",
    "    \"\"\"\n",
    "    xnew = np.array(x) + np.array(u)\n",
    "    xnew = tuple(xnew)\n",
    "    if state_consistency_check(env,xnew):\n",
    "        return xnew, True\n",
    "    return x, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "A974zRiTUpWt"
   },
   "outputs": [],
   "source": [
    "def probabilistic_transition_function(env,x,u, epsilon = 0.6):\n",
    "    \"\"\"Probabilistic Transition function requires:\n",
    "    x: current state, this is a tuple (i,j)\n",
    "    u: current action, this is a tuple (i,j)\n",
    "    env: enviroment\n",
    "    epsilon (in [0,1]): This is the probability of carrying out the desired action, in the extreme, 1 indicates a perfect action execution.\n",
    "\n",
    "    Output:\n",
    "    state_propagated_list: list of propagated states\n",
    "    prob_list: list of the corresponding state's prob, in the same order\n",
    "    \"\"\"\n",
    "    state_propagated_list = []\n",
    "    prob_list = []\n",
    "    for action in action_space:\n",
    "        xnew = np.array(x) + np.array(action)\n",
    "        xnew = tuple(xnew)\n",
    "        prob = (1-epsilon)/3\n",
    "        if action == u:\n",
    "            prob = epsilon\n",
    "        state_propagated_list.append(xnew)\n",
    "        prob_list.append(prob)\n",
    "    return state_propagated_list, prob_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 1A** (2 pts) Enumerate the action space. The coordinates of actions are u = (row, column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RoOLXnoL4i7-",
    "outputId": "f28ce577-49f9-4d12-def7-d8878a40b457"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The list of actions are:  [(-1, 0), (0, -1), (1, 0), (0, 1)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "import mdp\n",
    "import vi\n",
    "\n",
    "print(\"The list of actions are: \",action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-elqEvv83zB"
   },
   "source": [
    "**TASK 1B**  (3 pts) Formulate mathematically the optimal cost-to-go G∗\n",
    "(x) in recursive form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tkknhzv7DQya"
   },
   "source": [
    "To formulate the optimal cost-to-go function $G(x)$ in recursive form, we can use the Bellman equation. The Bellman equation expresses the optimal value of being in state x as the maximum value of the immediate cost plus the minimum value of the expected cost-to-go from the next states.\n",
    "\n",
    "Mathematically, the optimal cost-to-go $G(x)$ is defined as:\n",
    "\n",
    "$G(x) = min [c(x, u) + ∑ P_r ({x' | x, u}) G(x')]$\n",
    "\n",
    "where:\n",
    "- $G(x)$ is the optimal cost-to-go from state $x$.\n",
    "- $c(x, u)$ is the immediate cost of taking action $u$ in state $x$.\n",
    "- $P_r({x' | x, u})$ is the probability of transitioning to state $x'$ from state $x$ when taking action $u$.\n",
    "- $G(x')$ is the optimal cost-to-go from state $x'$.\n",
    "\n",
    "This recursive equation represents the optimal cost-to-go as the minimum cost of taking an action in the current state, plus the expected cost-to-go from all possible next states weighted by their transition probabilities.\n",
    "\n",
    "To calculate the optimal cost-to-go for all states, we can iterate over all states $x$ and update $G(x)$ using the Bellman equation until it converges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Cv7-UJ3EUtK"
   },
   "source": [
    "**TASK 1C**  (20 pts) Implement the VI algorithm for infinite length sequences. To show this, you are asked to include a\n",
    "picture of the final G∗\n",
    "(using imshow() for instance).\n",
    "The cost of traversing each node l(x, u) = 1 only if propagation is possible (there is not obstacle or out of\n",
    "bounds).\n",
    "Hint: You can implement a convergence criterion or simply run for a large number of iteration, say 100.\n",
    "Both options will be correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "IOd2jMswyeN7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from utils import plot_enviroment, action_space, transition_function\n",
    "from vi import vi, policy_vi\n",
    "from mdp import mdp, policy_mdp\n",
    "\n",
    "data = np.load('data_ps3.npz')\n",
    "environment = data['environment']\n",
    "\n",
    "x_ini = (11,6)\n",
    "goal = (15,29)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "GwM1AfAaETr0"
   },
   "outputs": [],
   "source": [
    "def transition_function(environment,x,u):\n",
    "    \"\"\"Transition function for states in this problem\n",
    "    x: current state, this is a tuple (i,j)\n",
    "    u: current action, this is a tuple (i,j)\n",
    "    env: enviroment\n",
    "\n",
    "    Output:\n",
    "    new state\n",
    "    True if correctly propagated\n",
    "    False if this action can't be executed\n",
    "    \"\"\"\n",
    "    xnew = np.array(x) + np.array(u)\n",
    "    xnew = tuple(xnew)\n",
    "    if state_consistency_check(environment,xnew):\n",
    "        return xnew, True\n",
    "    return x, False\n",
    "\n",
    "def vi(env, goal, gamma=1.0, epsilon=1e-6, max_iterations=100):\n",
    "    \"\"\"\n",
    "    Value Iteration algorithm for infinite-length sequences.\n",
    "    env: The grid environment.\n",
    "    goal: The goal state as (x, y) coordinates.\n",
    "    gamma: Discount factor.\n",
    "    epsilon: Convergence threshold.\n",
    "    max_iterations: Maximum number of iterations.\n",
    "\n",
    "    Returns the optimal cost-to-go matrix G.\n",
    "    \"\"\"\n",
    "    GRID_SIZE = env.shape\n",
    "    action_space = [(0, 1), (0, -1), (1, 0), (-1, 0)]  \n",
    "    G = np.zeros(GRID_SIZE) \n",
    "    for _ in range(max_iterations):\n",
    "        old_G = np.copy(G)\n",
    "        for i in range(GRID_SIZE[0]):\n",
    "            for j in range(GRID_SIZE[1]):\n",
    "                if (i, j) == goal:\n",
    "                    continue  # Skip the goal state\n",
    "                actions_cost = []\n",
    "                for u in action_space:\n",
    "                    xnew, success = transition_function(env, (i, j), u)\n",
    "                    if success:\n",
    "                        xnew_i, xnew_j = xnew\n",
    "                        next_cost = 1.0  # Cost of traversal\n",
    "                        next_cost += gamma * old_G[xnew_i, xnew_j]  # Future cost-to-go\n",
    "                        actions_cost.append(next_cost)\n",
    "                if actions_cost:\n",
    "                    G[i, j] = np.min(actions_cost)\n",
    "        if np.max(np.abs(G - old_G)) < epsilon:\n",
    "            break\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "o7k__GTUfmFM",
    "outputId": "ab7be880-ef9c-4c9d-c919-892e8617352d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "G = vi(environment, goal)\n",
    "plt.imshow(G, cmap='hot', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title(\"Optimal Cost-to-Go\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIuf9REC7kuM"
   },
   "source": [
    "**Task 1D**  (5 pts) Experiment with different number of iterations. Start with a 1 iteration VI, describe the results\n",
    "obtained and reason why.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FY5oMpIw-rMC"
   },
   "source": [
    "**Comment**  If we run the Value Iteration algorithm with only 1 iteration, the result would not be optimal. Value Iteration is an iterative algorithm that updates the cost-to-go values based on the values of neighboring states. Each iteration refines the cost-to-go estimates until convergence is reached.\n",
    "\n",
    "With only 1 iteration, the algorithm would only make one update to the cost-to-go values based on the neighboring states. This means that the algorithm would not have enough iterations to propagate the values effectively and accurately estimate the optimal cost-to-go for each state.\n",
    "\n",
    "As a result, the obtained cost-to-go values would not accurately represent the true optimal cost-to-go values. The path planning or decision-making based on these values would likely be suboptimal.\n",
    "\n",
    "In summary, running Value Iteration with only 1 iteration would not produce the optimal solution and would not effectively estimate the cost-to-go values. More iterations are needed to refine and converge the cost-to-go estimates towards the true optimal values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCUUrTs7-yxU"
   },
   "source": [
    "**Task 2A**  (3 pts) Formulate how to obtain the optimal policy $u^\n",
    "∗ = π_{V I} (x)$ from $G^∗$\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5c5p5FJR_h5e"
   },
   "source": [
    "To obtain the optimal policy from the optimal cost-to-go matrix $G^∗$, we can follow these steps:\n",
    "\n",
    "1. Initialize an empty policy matrix $π$, with the same shape as $G^∗$.\n",
    "\n",
    "2. For each state $(i, j)$ in the environment:\n",
    "   - Initialize an empty list actions_cost.\n",
    "   - For each possible action $u$ in the action_space:\n",
    "     - Compute the new state xnew and the success flag by applying the transition_function with $(i, j)$ and u as inputs.\n",
    "     - If success: Calculate the cost of traversal next_cost as $1.0$ plus the discounted cost-to-go value gamma $* G∗[x_{new}[0], x_{new}[1]]$.\n",
    "     - Append the action-cost pair $(u, next_{cost})$ to the actions_cost list.\n",
    "\n",
    "3. For each state $(i, j)$ in the environment:\n",
    "   - If it is the goal state, set the policy $π[i, j]$ to None.\n",
    "   - If it is not the goal state, set the policy $π[i, j]$ to the action $u$ that minimizes the cost next_cost in the actions_cost list.\n",
    "\n",
    "4. Return the policy $π$, which represents the optimal action to take from each state in order to maximize the expected cumulative reward.\n",
    "\n",
    "In summary, we iterate through each state, find the action that leads to the minimum cost of traversal plus the discounted cost-to-go value, and store it in the policy matrix. The resulting policy matrix provides the optimal action to take from each state to achieve the maximum expected cumulative reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKhFWLjGZSe3"
   },
   "source": [
    "**Task 2B**  . (17 pts) Implement an algorithm to obtain the optimal policy $u^\n",
    "∗$\n",
    "from $G^∗$\n",
    ". This policy can be a table. To\n",
    "test this, start at an initial position and execute the result of your policy and the transition function until\n",
    "you reach the goal. You will upload the video, that should be automatically generated if using the code in\n",
    "run.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "PDio4RwT0N4F"
   },
   "outputs": [],
   "source": [
    "def policy_vi(environment, G, action_space):\n",
    "    \"\"\"\n",
    "    environment: grid environment\n",
    "    G: optimal cost-to-go function\n",
    "    action_space: list of possible actions\n",
    "    output:\n",
    "    policy: a map from each state x to the best action u to execute\n",
    "    \"\"\"\n",
    "    rows, cols = environment.shape\n",
    "    policy = np.empty(environment.shape, dtype=object)\n",
    "\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            if environment[i, j] == 1:\n",
    "                policy[i, j] = None\n",
    "            else:\n",
    "                actions_cost = []\n",
    "                for u in action_space:\n",
    "                    xnew, success = transition_function(environment, (i, j), u)\n",
    "                    if success:\n",
    "                        next_cost = 1.0 + G[xnew[0], xnew[1]]\n",
    "                        actions_cost.append((u, next_cost))\n",
    "                min_cost_action = min(actions_cost, key=lambda x: x[1])[0]\n",
    "                policy[i, j] = min_cost_action\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Sk92WKaM0S94"
   },
   "outputs": [],
   "source": [
    "def execute_policy(policy, x_ini , environment, transition_function):\n",
    "    \"\"\"\n",
    "    policy: the optimal policy obtained\n",
    "    initial_state: the starting state\n",
    "    environment: grid environment\n",
    "    transition_function: function to compute the next state given a state-action pair\n",
    "    output:\n",
    "    traj: a list of states, representing the trajectory taken using the policy\n",
    "    \"\"\"\n",
    "    state = x_ini\n",
    "    traj = [state]\n",
    "    while policy[state[0], state[1]] is not None:\n",
    "        action = policy[state[0], state[1]]\n",
    "        state, success = transition_function(environment, state, action)\n",
    "        if not success:\n",
    "            print(\"Failed to execute action:\", action)\n",
    "            break\n",
    "        traj.append(state)\n",
    "\n",
    "    return traj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ulIUKjJlvk1D"
   },
   "outputs": [],
   "source": [
    "if 0:\n",
    "    fig = plt.figure()\n",
    "    imgs = []\n",
    "    x = x_ini\n",
    "    for plan_iters in range(100):\n",
    "        im = plot_enviroment(environment,x,goal)\n",
    "        plot = plt.imshow(im)\n",
    "        imgs.append([plot])\n",
    "        if x == goal:\n",
    "            print('Goal achieved in iters =', plan_iters)\n",
    "            break\n",
    "    im = plot_enviroment(environment,x,goal)\n",
    "    plot = plt.imshow(im)\n",
    "    imgs.append([plot])\n",
    "    ani = animation.ArtistAnimation(fig, imgs, interval=100, blit=True)\n",
    "    ani.save('plan_vi.mp4')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "0dNzq5elPqEH"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def draw_grid(environment, cell_size):\n",
    "    \"\"\"Draws the grid environment\"\"\"\n",
    "    rows, cols = environment.shape\n",
    "    grid = np.zeros((rows * cell_size, cols * cell_size, 3), dtype=np.uint8)\n",
    "\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            if environment[i, j] == 1:\n",
    "                color = (0, 255, 0)  # Green color for goal state\n",
    "            elif environment[i, j] == 2:\n",
    "                color = (0, 0, 255)  # Red color for obstacles\n",
    "            else:\n",
    "                color = (255, 255, 255)  # White color for empty cells\n",
    "\n",
    "            grid[i*cell_size: (i+1)*cell_size, j*cell_size: (j+1)*cell_size] = color\n",
    "\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "UWhALGqwnpNP"
   },
   "outputs": [],
   "source": [
    "def visualize_policy(policy, initial_state, environment, transition_function, cell_size=20, delay=500,video_filename=\"output.mp4\"):\n",
    "    \"\"\"\n",
    "    policy: the optimal policy obtained\n",
    "    initial_state: the starting state\n",
    "    environment: grid environment\n",
    "    transition_function: function to compute the next state given a state-action pair\n",
    "    cell_size: size of each cell in pixels\n",
    "    delay: delay between each frame in milliseconds\n",
    "    output:\n",
    "    frame_count: number of frames in the video\n",
    "    \"\"\"\n",
    "    state = initial_state\n",
    "    frames = []\n",
    "    while policy[state[0], state[1]] is not None:\n",
    "        grid = draw_grid(environment, cell_size)\n",
    "        action = policy[state[0], state[1]]\n",
    "        state, success = transition_function(environment, state, action)\n",
    "        if not success:\n",
    "            print(\"Failed to execute action:\", action)\n",
    "            break\n",
    "        x, y = state\n",
    "        cv2.rectangle(grid, (y*cell_size, x*cell_size), ((y+1)*cell_size, (x+1)*cell_size), (0, 0, 255), -1)\n",
    "        frames.append(grid.copy())\n",
    "        cv2.imshow(\"Policy Video\",grid)\n",
    "        if cv2.waitKey(delay) & 0xFF == ord('q'):\n",
    "            break\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.Video\n",
    "    cv2.destroyAllWindows()\n",
    "    return len(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "mYOtOfDfbFr8",
    "outputId": "2676978d-1103-4437-81c5-1dcb1e0c3ffa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frames: 33\n"
     ]
    }
   ],
   "source": [
    "environment = data['environment']\n",
    "policy = policy_vi(environment, G, action_space)\n",
    "frame_count = visualize_policy(policy, x_ini, environment, transition_function, cell_size=50, delay=1000)\n",
    "print(\"Total frames:\", frame_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2C** (5 pts) Experiment with different parameters, such as starting points, the order of the states you use in VI (for loops) and the order of the actions. Explain your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ini = (11,6)\n",
    "action_space = [(0, 1), (0, -1), (-1, 0),(1, 0)] \n",
    "goal = (15,29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_function(environment,x,u):\n",
    "    \"\"\"Transition function for states in this problem\n",
    "    x: current state, this is a tuple (i,j)\n",
    "    u: current action, this is a tuple (i,j)\n",
    "    env: enviroment\n",
    "\n",
    "    Output:\n",
    "    new state\n",
    "    True if correctly propagated\n",
    "    False if this action can't be executed\n",
    "    \"\"\"\n",
    "    xnew = np.array(x) + np.array(u)\n",
    "    xnew = tuple(xnew)\n",
    "    if state_consistency_check(environment,xnew):\n",
    "        return xnew, True\n",
    "    return x, False\n",
    "\n",
    "def vi(env, goal, gamma=1.0, epsilon=1e-6, max_iterations=100):\n",
    "    \"\"\"\n",
    "    Value Iteration algorithm for infinite-length sequences.\n",
    "    env: The grid environment.\n",
    "    goal: The goal state as (x, y) coordinates.\n",
    "    gamma: Discount factor.\n",
    "    epsilon: Convergence threshold.\n",
    "    max_iterations: Maximum number of iterations.\n",
    "\n",
    "    Returns the optimal cost-to-go matrix G.\n",
    "    \"\"\"\n",
    "    GRID_SIZE = env.shape\n",
    "    action_space = [(0, 1), (0, -1), (-1, 0),(1, 0)]  \n",
    "    G = np.zeros(GRID_SIZE) \n",
    "    for _ in range(max_iterations):\n",
    "        old_G = np.copy(G)\n",
    "        for i in range(GRID_SIZE[0]):\n",
    "            for j in range(GRID_SIZE[1]):\n",
    "                if (i, j) == goal:\n",
    "                    continue  # Skip the goal state\n",
    "                actions_cost = []\n",
    "                for u in action_space:\n",
    "                    xnew, success = transition_function(env, (i, j), u)\n",
    "                    if success:\n",
    "                        xnew_i, xnew_j = xnew\n",
    "                        next_cost = 1.0  # Cost of traversal\n",
    "                        next_cost += gamma * old_G[xnew_i, xnew_j]  # Future cost-to-go\n",
    "                        actions_cost.append(next_cost)\n",
    "                if actions_cost:\n",
    "                    G[i, j] = np.min(actions_cost)\n",
    "        if np.max(np.abs(G - old_G)) < epsilon:\n",
    "            break\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "G = vi(environment, goal)\n",
    "plt.imshow(G, cmap='hot', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title(\"Optimal Cost-to-Go\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_vi(environment, G, action_space):\n",
    "    \"\"\"\n",
    "    environment: grid environment\n",
    "    G: optimal cost-to-go function\n",
    "    action_space: list of possible actions\n",
    "    output:\n",
    "    policy: a map from each state x to the best action u to execute\n",
    "    \"\"\"\n",
    "    cols, rows = environment.shape\n",
    "    policy = np.empty(environment.shape, dtype=object)\n",
    "\n",
    "    for i in range(cols):\n",
    "        for j in range(rows):\n",
    "            if environment[i, j] == 1:\n",
    "                policy[i, j] = None\n",
    "            else:\n",
    "                actions_cost = []\n",
    "                for u in action_space:\n",
    "                    xnew, success = transition_function(environment, (i, j), u)\n",
    "                    if success:\n",
    "                        next_cost = 1.0 + G[xnew[0], xnew[1]]\n",
    "                        actions_cost.append((u, next_cost))\n",
    "                min_cost_action = min(actions_cost, key=lambda x: x[1])[0]\n",
    "                policy[i, j] = min_cost_action\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_policy(policy, x_ini , environment, transition_function):\n",
    "    \"\"\"\n",
    "    policy: the optimal policy obtained\n",
    "    initial_state: the starting state\n",
    "    environment: grid environment\n",
    "    transition_function: function to compute the next state given a state-action pair\n",
    "    output:\n",
    "    traj: a list of states, representing the trajectory taken using the policy\n",
    "    \"\"\"\n",
    "    state = x_ini\n",
    "    traj = [state]\n",
    "    while policy[state[0], state[1]] is not None:\n",
    "        action = policy[state[0], state[1]]\n",
    "        state, success = transition_function(environment, state, action)\n",
    "        if not success:\n",
    "            print(\"Failed to execute action:\", action)\n",
    "            break\n",
    "        traj.append(state)\n",
    "\n",
    "    return traj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    fig = plt.figure()\n",
    "    imgs = []\n",
    "    x = x_ini\n",
    "    for plan_iters in range(100):\n",
    "        im = plot_enviroment(environment,x,goal)\n",
    "        plot = plt.imshow(im)\n",
    "        imgs.append([plot])\n",
    "        if x == goal:\n",
    "            print('Goal achieved in iters =', plan_iters)\n",
    "            break\n",
    "    im = plot_enviroment(environment,x,goal)\n",
    "    plot = plt.imshow(im)\n",
    "    imgs.append([plot])\n",
    "    ani = animation.ArtistAnimation(fig, imgs, interval=100, blit=True)\n",
    "    ani.save('plan_vi.mp4')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def draw_grid(environment, cell_size):\n",
    "    \"\"\"Draws the grid environment\"\"\"\n",
    "    cols, rows = environment.shape\n",
    "    grid = np.zeros((cols * cell_size, rows * cell_size, 3), dtype=np.uint8)\n",
    "\n",
    "    for i in range(cols):\n",
    "        for j in range(rows):\n",
    "            if environment[i, j] == 1:\n",
    "                color = (0, 255, 0)  # Green color for goal state\n",
    "            elif environment[i, j] == 2:\n",
    "                color = (0, 0, 255)  # Red color for obstacles\n",
    "            else:\n",
    "                color = (255, 255, 255)  # White color for empty cells\n",
    "\n",
    "            grid[i*cell_size: (i+1)*cell_size, j*cell_size: (j+1)*cell_size] = color\n",
    "\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_policy(policy, initial_state, environment, transition_function, cell_size=20, delay=500,video_filename=\"output.mp4\"):\n",
    "    \"\"\"\n",
    "    policy: the optimal policy obtained\n",
    "    initial_state: the starting state\n",
    "    environment: grid environment\n",
    "    transition_function: function to compute the next state given a state-action pair\n",
    "    cell_size: size of each cell in pixels\n",
    "    delay: delay between each frame in milliseconds\n",
    "    output:\n",
    "    frame_count: number of frames in the video\n",
    "    \"\"\"\n",
    "    state = initial_state\n",
    "    frames = []\n",
    "    while policy[state[0], state[1]] is not None:\n",
    "        grid = draw_grid(environment, cell_size)\n",
    "        action = policy[state[0], state[1]]\n",
    "        state, success = transition_function(environment, state, action)\n",
    "        if not success:\n",
    "            print(\"Failed to execute action:\", action)\n",
    "            break\n",
    "        y, x = state\n",
    "        cv2.rectangle(grid, (x*cell_size, y*cell_size), ((x+1)*cell_size, (y+1)*cell_size), (0, 0, 255), -1)\n",
    "        frames.append(grid.copy())\n",
    "        cv2.imshow(\"Policy Video\",grid)\n",
    "        if cv2.waitKey(delay) & 0xFF == ord('q'):\n",
    "            break\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.Video\n",
    "    cv2.destroyAllWindows()\n",
    "    return len(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'Video'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_44592/1186175200.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0menvironment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'environment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy_vi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menvironment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mframe_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvisualize_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_ini\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menvironment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransition_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Total frames:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_44592/2156173707.py\u001b[0m in \u001b[0;36mvisualize_policy\u001b[1;34m(policy, initial_state, environment, transition_function, cell_size, delay, video_filename)\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mfourcc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVideoWriter_fourcc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;34m\"mp4v\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVideo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'Video'"
     ]
    }
   ],
   "source": [
    "environment = data['environment']\n",
    "policy = policy_vi(environment, G, action_space)\n",
    "frame_count = visualize_policy(policy, x_ini, environment, transition_function, cell_size=50, delay=1000)\n",
    "print(\"Total frames:\", frame_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "1. Starting Points: By changing the starting point, you can observe different trajectories being generated by executing the policy. The trajectory will follow the optimal path determined by the Value Iteration algorithm from the new starting point to the goal state.\n",
    "\n",
    "2. Order of States: The order in which the states are visited during the Value Iteration algorithm does not affect the final optimal cost-to-go or policy since all states are iteratively updated based on their neighbors' values. The algorithm will converge to the same solution regardless of the order in which the states are processed.\n",
    "\n",
    "3. Order of Actions: The order in which the actions are considered during the Value Iteration algorithm can have a slight impact on the convergence speed, but it should not affect the final solution significantly. Changing the order of actions in each state affects the exploration of the state-action space, potentially leading to different convergence rates or slightly different policies.\n",
    "\n",
    "Overall, the provided code implements the Value Iteration algorithm, extracts the optimal policy, and allows for experiments with different starting points, state orders, and action orders. These experiments help understand how these parameters can impact the trajectories and convergence rates of the algorithm in different scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3A** (3 pts) Formulate the optimal value function $v_∗(x)$ in recursive form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal value function in recursive form can be formulated as follows:\n",
    "\n",
    "$v_∗(x) = max(u(x, a) + βv_∗(f(x, a)))$\n",
    "\n",
    "where:\n",
    "- $v_∗(x)$ is the optimal value function for state $x$\n",
    "- $u(x, a)$ is the immediate reward function for taking action $a$ in state $x$\n",
    "- $f(x, a)$ is the transition function that returns the next state given current state $x$ and action $a$\n",
    "- $β$ is the discount factor that represents the importance of future rewards\n",
    "\n",
    "In this formulation, we take the maximum value of the immediate reward plus the discounted future value for each possible action $a$, and we recursively calculate the optimal value function for the next state $f(x, a)$ until we reach a terminal state. The optimal value function $v_∗(x)$ represents the maximum expected cumulative reward that can be obtained starting from state $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3B**  (2 pts) Formulate how to obtain the greedy deterministic policy $u^∗$ = $π_{MDP}(x)$ from $v_∗(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the greedy deterministic policy $u^∗$ = $π_{MDP} (x)$ from v_∗(x), we need to select the action $a$ in each state $x$ that maximizes the expression:\n",
    "\n",
    "$u(x, a)$ + $βv_∗(f(x, a))$\n",
    "\n",
    "In other words, the greedy policy chooses the action that yields the highest immediate reward plus the discounted future value. \n",
    "\n",
    "For each state $x$, we evaluate this expression for all possible actions $a$ and select the action that gives the maximum value. This action is then chosen as the policy for that state, resulting in a deterministic policy $u^∗$.\n",
    "\n",
    "In mathematical notation, the greedy deterministic policy $u^∗$ = $π_{MDP} (x)$ can be formulated as:\n",
    "\n",
    "$u^∗(x) = argmax(u(x, a) + βv∗(f(x, a)))$\n",
    "\n",
    "where argmax is the function that returns the argument that maximizes the expression. This formulation ensures that the policy $u^∗$ selects the action that leads to the highest expected cumulative reward based on the optimal value function $v_∗$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
